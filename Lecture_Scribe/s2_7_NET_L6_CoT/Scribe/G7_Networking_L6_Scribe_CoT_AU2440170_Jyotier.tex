\documentclass[12pt]{article}

\usepackage{amsmath, amssymb}
\usepackage{geometry}
\geometry{margin=1in}

\title{CSE400 – Fundamentals of Probability in Computing\\
Lecture 6: Discrete Random Variables, Expectation and Problem Solving}
\author{Instructor: Dhaval Patel, PhD}
\date{January 22, 2025}

\begin{document}

\maketitle

\section{Random Variables}

\subsection{Definition and Concept}

A random variable is defined as a function that assigns a real number to each outcome in the sample space.

Formally,

\[
X : \Omega \rightarrow \mathbb{R}
\]

where:

\begin{itemize}
\item $\Omega$ denotes the sample space.
\item $\omega \in \Omega$ denotes a sample point.
\item $X(\omega)$ is the real number assigned to the outcome $\omega$.
\end{itemize}

Thus, a random variable maps outcomes of an experiment to numerical values.

The lecture further restricts attention to discrete random variables, meaning:

\[
\{ X(\omega) : \omega \in \Omega \}
\]

is a finite or countably infinite subset of $\mathbb{R}$.

This means that although the codomain of $X$ is $\mathbb{R}$, the actual values taken by $X$ belong to a discrete set.

\subsection{Visualization of a Random Variable}

The distribution of a discrete random variable can be visualized using a bar diagram, where:

\begin{itemize}
\item The x-axis represents the possible values of the random variable.
\item The height of each bar at value $a$ represents:
\end{itemize}

\[
Pr(X = a)
\]

This probability corresponds to the probability of the event in the sample space mapped to value $a$.

\section{Types of Random Variables}

Random variables are classified into two main types.

\subsection{Discrete Random Variable}

A random variable is discrete if:

\begin{itemize}
\item It has countable support
\item A Probability Mass Function (PMF) exists
\item Probabilities are assigned to individual values
\item Each possible value has strictly positive probability
\end{itemize}

\subsection{Continuous Random Variable}

A random variable is continuous if:

\begin{itemize}
\item It has uncountable support
\item It uses a Probability Density Function (PDF)
\item Probabilities are assigned to intervals
\item Each individual value has zero probability
\end{itemize}

\section{Example of a Discrete Random Variable}

\subsection{Experiment Description}

Suppose an experiment consists of tossing 3 fair coins.

Define:

\[
Y = \text{number of heads}
\]

Then the possible values of $Y$ are:

\[
Y \in \{0,1,2,3\}
\]

\subsection{Probability Calculation}

All possible outcomes:

\[
\{TTT, TTH, THT, HTT, THH, HTH, HHT, HHH\}
\]

Each outcome has probability:

\[
\frac{1}{8}
\]

Case 1: No heads

\[
P(Y = 0) = P(TTT) = \frac{1}{8}
\]

Case 2: One head

Outcomes:

\[
TTH, THT, HTT
\]

\[
P(Y = 1) = \frac{3}{8}
\]

Case 3: Two heads

Outcomes:

\[
THH, HTH, HHT
\]

\[
P(Y = 2) = \frac{3}{8}
\]

Case 4: Three heads

Outcome:

\[
HHH
\]

\[
P(Y = 3) = \frac{1}{8}
\]

\subsection{Total Probability Property}

Since $Y$ must take one of these values:

\[
1 = \sum_{i=0}^{3} P(Y = i)
\]

\[
= \frac{1}{8} + \frac{3}{8} + \frac{3}{8} + \frac{1}{8}
\]

\[
= 1
\]

This confirms a valid probability distribution.

\section{Probability Mass Function (PMF)}

\subsection{Definition}

A random variable that takes at most a countable number of values is called discrete.

Let:

\[
R_X = \{x_1, x_2, x_3, \dots \}
\]

be the range of possible values.

The function:

\[
P_X(x_k) = P(X = x_k)
\]

for

\[
k = 1,2,3,\dots
\]

is called the Probability Mass Function (PMF).

\subsection{Property of PMF}

Since the random variable must take one of its possible values:

\[
\sum_{k=1}^{\infty} P_X(x_k) = 1
\]

This expresses the total probability law.

\section{PMF Example with Unknown Constant}

Given:

\[
p(i) = c \frac{\lambda^i}{i!}, \quad i = 0,1,2,\dots
\]

where $\lambda > 0$

Find:

\[
P(X=0)
\]

\[
P(X>2)
\]

\subsection{Step 1: Use Total Probability Property}

Since:

\[
\sum_{i=0}^{\infty} p(i) = 1
\]

Substitute:

\[
\sum_{i=0}^{\infty} c \frac{\lambda^i}{i!} = 1
\]

Factor $c$:

\[
c \sum_{i=0}^{\infty} \frac{\lambda^i}{i!} = 1
\]

Using the identity:

\[
\sum_{i=0}^{\infty} \frac{\lambda^i}{i!} = e^{\lambda}
\]

Thus:

\[
c e^{\lambda} = 1
\]

Solve for $c$:

\[
c = e^{-\lambda}
\]

\subsection{Step 2: Find $P(X=0)$}

Substitute $i=0$:

\[
P(X=0) = c \frac{\lambda^0}{0!}
\]

\[
= e^{-\lambda}
\]

\subsection{Step 3: Find $P(X>2)$}

Use complement:

\[
P(X>2) = 1 - P(X \leq 2)
\]

\[
= 1 - [P(X=0) + P(X=1) + P(X=2)]
\]

Substitute:

\[
= 1 - \left(e^{-\lambda} + \lambda e^{-\lambda} + \frac{\lambda^2}{2} e^{-\lambda} \right)
\]

\section{Bayes’ Theorem}

Using conditional probability:

\[
Pr(A|B_i) =
\frac{Pr(B_i|A) Pr(A)}
{\sum_{i=1}^{n} Pr(A|B_i) Pr(B_i)}
\]

This is called the Bayes Formula.

\subsection{Definitions}

Prior Probability $Pr(B_i)$

Probability formed before observing evidence.

Posterior Probability $Pr(B_i | A)$

Probability after observing event $A$.

\section{Bayes’ Theorem Example: Auditorium with 30 Rows}

Given

Auditorium has 30 rows

Row 1: 11 seats

Row 2: 12 seats

Row 3: 13 seats

...

Row 30: 40 seats

Selection process:

Select row randomly:

\[
P(R_k) = \frac{1}{30}
\]

Select seat randomly within row.

\subsection{Step 1: Find $P(S_{15} | R_{20})$}

Row 20 has:

\[
20 + 10 = 30 \text{ seats}
\]

Thus:

\[
P(S_{15} | R_{20}) = \frac{1}{30}
\]

\subsection{Step 2: Find $P(R_{20} | S_{15})$}

Using Bayes formula:

\[
P(R_{20} | S_{15}) =
\frac{P(S_{15} | R_{20}) P(R_{20})}
{P(S_{15})}
\]

Denominator:

\[
P(S_{15}) =
\sum_{k=5}^{30}
P(S_{15} | R_k) P(R_k)
\]

\[
=
\sum_{k=5}^{30}
\frac{1}{k+10}
\frac{1}{30}
\]

Numerical value:

\[
= 0.0342
\]

Thus:

\[
P(R_{20} | S_{15})
=
\frac{(1/30)(1/30)}{0.0342}
\]

\[
= 0.0325
\]

\section{Summary of Key Results}

Random variable definition:

\[
X : \Omega \rightarrow \mathbb{R}
\]

PMF definition:

\[
P_X(x_k) = P(X = x_k)
\]

PMF property:

\[
\sum P_X(x_k) = 1
\]

Constant value:

\[
c = e^{-\lambda}
\]

Bayes theorem:

\[
P(B_i | A)
=
\frac{P(A | B_i) P(B_i)}
{\sum P(A | B_i) P(B_i)}
\]

\end{document}
 